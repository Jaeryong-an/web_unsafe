import os, re, time, datetime, requests, base64, io
import streamlit as st
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from google.oauth2.service_account import Credentials
from webdriver_manager.chrome import ChromeDriverManager
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from PIL import Image
import pytesseract
import gspread
import openai
from fugashi import Tagger
import json

# Googleèªè¨¼ã¨APIã‚­ãƒ¼
SERVICE_ACCOUNT_JSON = os.getenv("SERVICE_ACCOUNT_JSON")
SPREADSHEET_ID = os.getenv("SPREADSHEET_ID")
SHEET_NAME = os.getenv("SHEET_NAME")
DRIVE_FOLDER_ID = os.getenv("DRIVE_FOLDER_ID")
GPT_API_KEY = os.getenv("OPENAI_API_KEY")

# âœ… í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ ì²´í¬
missing_vars = []
for var_name in ["SERVICE_ACCOUNT_JSON", "SPREADSHEET_ID", "SHEET_NAME", "DRIVE_FOLDER_ID", "OPENAI_API_KEY"]:
    if not os.getenv(var_name):
        missing_vars.append(var_name)
if missing_vars:
    st.error(f"âŒ æ¬¡ã®å¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã¾ã›ã‚“ã€‚: {', '.join(missing_vars)}")
    st.stop()

# âœ… ì¸ì¦ ì²˜ë¦¬
try:
    service_account_info = json.loads(SERVICE_ACCOUNT_JSON)
except Exception:
    st.error("âŒ SERVICE_ACCOUNT_JSON ãŒæœ‰åŠ¹ãªJSONã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    st.stop()

creds = Credentials.from_service_account_info(
    service_account_info,
    scopes=[
        "https://www.googleapis.com/auth/drive",
        "https://www.googleapis.com/auth/spreadsheets"
    ]
)

gc = gspread.authorize(creds)
drive_service = build('drive', 'v3', credentials=creds)
worksheet = gc.open_by_key(SPREADSHEET_ID).worksheet(SHEET_NAME)

# âœ… OpenAI ì„¤ì •
openai.api_key = GPT_API_KEY

@st.cache_resource
def validate_openai_key(api_key):
    if not api_key:
        return False
    try:
        headers = {"Authorization": f"Bearer {api_key}"}
        resp = requests.get("https://api.openai.com/v1/models", headers=headers, timeout=5)
        return resp.status_code == 200
    except Exception:
        return False

def load_rules_from_sheet(sheet_name: str):
    worksheet = gc.open_by_key(SPREADSHEET_ID).worksheet(sheet_name)
    rows = worksheet.get_all_values()[1:] 

    genre_keywords = {}
    url_patterns = {}
    japanese_domains = []

    for row in rows:
        if len(row) < 4:
            continue

        genre = row[0].strip()
        rule_type = row[1].strip().lower()
        rule_base = row[2].strip() if len(row) >= 3 else ""
        rule_regex = row[3].strip() if len(row) >= 4 else ""

        rule = rule_regex if rule_regex else rule_base
        if not rule:
            continue

        if rule_type == "keyword":
            genre_keywords.setdefault(genre, []).append(rule)
        elif rule_type == "pattern":
            url_patterns.setdefault(genre, []).append(rule)
        elif rule_type == "jpdomain":
            japanese_domains.append(rule.lower())

    return genre_keywords, url_patterns, japanese_domains

# OCR + ã‚¯ãƒªãƒ¼ãƒ³æœ¬æ–‡æŠ½å‡º
tagger = Tagger()

def get_words(text):
    return [word.surface for word in tagger(text)] if text else []

def extract_domain(url):
    try:
        return urlparse(url).netloc.lower()
    except:
        return ""

def fetch_with_retry(url, timeout=10, retries=1):
    for attempt in range(retries + 1):
        try:
            resp = requests.get(url, timeout=timeout)
            resp.encoding = resp.apparent_encoding
            return resp
        except requests.exceptions.RequestException:
            if attempt < retries:
                time.sleep(1)
    return None

def extract_clean_text(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    ad_selectors = [
        'header', 'footer', 'nav', 'iframe', 'ins',
        '.footer', '.header', '.ads', '.sponsor', '.promo', '.widget',
        '[id*="ad"]', '[class*="ad"]', '[class*="sponsor"]', '[id*="sponsor"]',
        '[class*="banner"]', '[id*="banner"]', '[class*="rec"]', '[class*="promo"]'
    ]
    for selector in ad_selectors:
        for tag in soup.select(selector):
            tag.decompose()

    for tag in soup(["script", "style"]):
        tag.decompose()

    return soup.get_text(separator="\n", strip=True)


GENRE_KEYWORDS, URL_PATTERNS, JAPANESE_DOMAINS = load_rules_from_sheet("GenreRules")

def extract_image_ocr_text(img_path: str) -> str:
    try:
        img = Image.open(img_path)
        return pytesseract.image_to_string(img, lang='jpn+eng').strip()
    except:
        return ""

# ã‚¹ã‚¯ã‚·ãƒ§
def take_fullpage_screenshot(driver, save_path):
    try:
        # JavaScriptã®èª­ã¿è¾¼ã¿å¾…ã¡ï¼ˆæœ€å¤§5ç§’ï¼‰
        for _ in range(5):
            ready_state = driver.execute_script("return document.readyState")
            if ready_state == "complete":
                break
            time.sleep(1)

        # ãƒšãƒ¼ã‚¸å…¨ä½“ã®ã‚µã‚¤ã‚ºå–å¾—
        total_height = driver.execute_script("return Math.max(document.body.scrollHeight, document.documentElement.scrollHeight)")
        total_width = driver.execute_script("return Math.max(document.body.scrollWidth, document.documentElement.scrollWidth)")

        driver.set_window_size(total_width, total_height)
        time.sleep(2)  # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒªã‚µã‚¤ã‚ºåæ˜ å¾…ã¡

        # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ã‹ã‚‰ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹ã“ã¨ã§èª­ã¿è¾¼ã¿ãŒå®‰å®š
        driver.execute_script("window.scrollTo(0, 0)")
        time.sleep(0.5)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight / 2)")
        time.sleep(0.5)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")
        time.sleep(0.5)
        driver.execute_script("window.scrollTo(0, 0)")
        time.sleep(1.5)

        driver.save_screenshot(save_path)
        if os.path.exists(save_path):
            return save_path
        else:
            raise FileNotFoundError("ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
    except Exception as e:
        st.warning(f"ğŸ“¸ ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return ""

def crawl_with_ocr(url: str, idx: int) -> tuple[str, str, str, str, str]:
    try:
        try:
            resp = requests.get(url, timeout=10)
            resp.encoding = resp.apparent_encoding
            maintext = extract_clean_text(resp.text)
        except Exception:
            maintext = ""

        shot_path = f"screenshot_{idx}.png"
        driver = None
        try:
            options = Options()
            options.add_argument('--headless') 
            options.add_argument('--disable-gpu')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--window-size=1280,1500') 
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)
            driver.set_page_load_timeout(20)
            driver.get(url)
            time.sleep(2.5)
            html = driver.page_source
            shot_path = take_fullpage_screenshot(driver, shot_path)

            if not maintext:
                maintext = extract_clean_text(driver.page_source)

            soup = BeautifulSoup(driver.page_source, "html.parser")
            img = soup.find("img")
            img_src = img["src"] if img and img.get("src") else ""
            img_alt = img["alt"] if img and img.get("alt") else ""
            image_desc = img_alt or img_src

        except Exception:
            image_desc = ""
        finally:
            if driver:
                driver.quit()

        ocr_text = extract_image_ocr_text(shot_path if os.path.exists(shot_path) else "")
        image_combined_desc = image_desc + "\n" + ocr_text

        if not maintext.strip():
            maintext = "æœ¬æ–‡å–å¾—å¤±æ•—"
        return maintext, image_combined_desc, shot_path, html if os.path.exists(shot_path) else "", ocr_text
    except Exception as e:
        return "æœ¬æ–‡å–å¾—å¤±æ•—", "", ""

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åŠã³ã‚¸ãƒ£ãƒ³ãƒ«åˆ¤å®š
def judge_keywords_by_count(text, keywords_dict):
    results = []
    for genre, patterns in keywords_dict.items():
        matched = []
        for pattern in patterns:
            try:
                if re.search(pattern, text, re.IGNORECASE):
                    matched.append(pattern)
            except re.error:
                continue  # ã‚¨ãƒ©ãƒ¼ç„¡è¦–
        if matched:
            results.append((genre, len(matched), matched))
    return results

def judge_genre_by_patterns(targets, patterns_dict):
    for genre, patterns in patterns_dict.items():
        for patt in patterns:
            patt_regex = re.escape(patt).replace(r'\*', '.*')
            for target in targets:
                if target and re.search(patt_regex, target, re.IGNORECASE):
                    return genre, patt
    return None, None

def judge_genre_final(url, text, image_desc, image_url, keywords_dict, url_patterns, gpt_fn=None):
    targets = [url, image_url, image_desc]
    genre, matched = judge_genre_by_patterns(targets, url_patterns)
    if genre:
        return f"[ãƒ‘ã‚¿ãƒ¼ãƒ³/ãƒ‰ãƒ¡ã‚¤ãƒ³ä¸€è‡´] {genre}ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³: {matched}ï¼‰"
    text_for_kw = (text or "") + " " + (image_desc or "")
    keyword_matches = judge_keywords_by_count(text_for_kw, keywords_dict)
    if keyword_matches:
        lines = []
        for genre, count, matched in keyword_matches:
            lines.append(f"{genre}ï¼ˆ{count}å€‹ä¸€è‡´ï¼‰ - {', '.join(matched[:5])}...")
        return "[ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ]\n" + "\n".join(lines)
    if gpt_fn:
        return f"[GPTåˆ¤å®š] {gpt_fn(text, image_desc, keywords_dict)}"
    return "åˆ¤å®šä¸å¯"

# --- GPT ---
def gpt_judge_genre(maintext, image_desc, keywords_dict):
    if not GPT_API_KEY:
        return "GPTæœªå®Ÿè¡Œ"
    maintext_short = maintext[:800] if len(maintext) > 800 else maintext
    image_desc_short = image_desc[:300] if len(image_desc) > 300 else image_desc
    full_context = f"[æœ¬æ–‡ï¼ˆæœ€å¤§800å­—ï¼‰]:\n{maintext_short}\n\n[ç”»åƒã®èª¬æ˜ãƒ»OCRçµæœï¼ˆæœ€å¤§300å­—ï¼‰]:\n{image_desc_short}"

    genre_prompt = (
    "ä»¥ä¸‹ã¯ã‚ã‚‹ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®æœ¬æ–‡ã¨ç”»åƒã«é–¢ã™ã‚‹æƒ…å ±ã§ã™ã€‚ä¸‹è¨˜ã®ã‚¸ãƒ£ãƒ³ãƒ«å®šç¾©ã«åŸºã¥ã„ã¦ã€ã©ã®ã‚«ãƒ†ã‚´ãƒªã«è©²å½“ã™ã‚‹ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚\n"
    "è¤‡æ•°è©²å½“ã—ã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚\n"
    "è©²å½“ãªã—ã®å ´åˆã¯ã€Œ[ã‚¸ãƒ£ãƒ³ãƒ«]: è¦ç¢ºèªã€ã¨è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚\n\n"
    "ã€ã‚¸ãƒ£ãƒ³ãƒ«å®šç¾©ã¨NG/OKä¾‹ã€‘\n"
    "1. ã‚¢ãƒ€ãƒ«ãƒˆï¼šR18æŒ‡å®šã€æ€§çš„ãªæå†™ã€è‚Œã®éœ²å‡ºãŒå¤šã„ã€å¹´é½¢åˆ¶é™ã®ã‚ã‚‹ãƒšãƒ¼ã‚¸\n"
    "   NGä¾‹: ãƒŒãƒ¼ãƒ‰ç”»åƒã€æ€§è¡Œç‚ºæå†™ã€å¹´é½¢ç¢ºèªãƒšãƒ¼ã‚¸ã‚ã‚Š\n"
    "   OKä¾‹: å¥åº·ã‚„åŒ»ç™‚ç›®çš„ã®æ€§æ•™è‚²ãƒšãƒ¼ã‚¸\n\n"
    "2. æ‚ªè³ªCGMï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼æŠ•ç¨¿å‹ã‚µã‚¤ãƒˆã§ä¸é©åˆ‡ãªã‚³ãƒ¡ãƒ³ãƒˆã‚„æ‚ªè³ªæŠ•ç¨¿ãŒã•ã‚Œã‚„ã™ã„ã‚‚ã®\n"
    "   NGä¾‹: 2ã¡ã‚ƒã‚“ã­ã‚‹ã€çˆ†ã‚µã‚¤ã€å‡ºä¼šã„ç³»æ²ç¤ºæ¿\n"
    "   OKä¾‹: é©åˆ‡ã«ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚Œã¦ã„ã‚‹å£ã‚³ãƒŸã‚µã‚¤ãƒˆ\n\n"
    "3. è‘—ä½œæ¨©ä¾µå®³ï¼šè‘—ä½œç‰©ï¼ˆæ¼«ç”»ãƒ»ã‚¢ãƒ‹ãƒ¡ãƒ»æ˜ ç”»ãƒ»éŸ³æ¥½ãªã©ï¼‰ã‚’ç„¡æ–­è»¢è¼‰ã—ã¦ã„ã‚‹ã‚µã‚¤ãƒˆ\n"
    "   NGä¾‹: æ¼«ç”»ã®ã‚¹ã‚¯ã‚·ãƒ§æ²è¼‰ã€ã‚¿ãƒ¬ãƒ³ãƒˆã®ç”»åƒå¤šæ•°è»¢è¼‰ã€éåº¦ãªãƒã‚¿ãƒãƒ¬\n"
    "   OKä¾‹: è¡¨ç´™ç”»åƒã®ã¿ã€æ„Ÿæƒ³ã®ã¿è¨˜è¼‰ã—ãŸãƒ–ãƒ­ã‚°\n\n"
    "4. ãƒã‚¤ãƒ³ãƒˆï¼šãƒã‚¤ãƒ³ãƒˆä»˜ä¸ã‚„äº¤æ›ãŒä¸»ç›®çš„ã®ã‚µã‚¤ãƒˆã€ã¾ãŸã¯ãã®ç´¹ä»‹\n"
    "   NGä¾‹: ãƒã‚¤ãƒ³ãƒˆãƒ¡ãƒ¼ãƒ«ã€ãƒã‚¤ãƒ³ãƒˆæƒ…å ±ãƒ»äº¤æ›ãƒ»ç´¹ä»‹ç³»ã‚µã‚¤ãƒˆ\n"
    "   OKä¾‹: é‡‘èå•†å“ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚µã‚¤ãƒˆï¼ˆãƒã‚¤ãƒ³ãƒˆéé‡è¦–ï¼‰\n\n"
    "5. ãƒ˜ã‚¤ãƒˆï¼šå€‹äººãƒ»äººç¨®ãƒ»å®—æ•™ãƒ»æ€§åˆ¥ãªã©ã«å¯¾ã™ã‚‹èª¹è¬—ä¸­å‚·ã‚„å·®åˆ¥çš„è¡¨ç¾\n"
    "   NGä¾‹: ç‰¹å®šé›†å›£ã¸ã®æ”»æ’ƒã€å·®åˆ¥ç”¨èªã®ä½¿ç”¨ã€è·æ¥­æ‰¹åˆ¤ã€æ”¿æ²»çš„ç…½å‹•\n"
    "   OKä¾‹: æ‰¹åˆ¤çš„ã ãŒå†·é™ãªå ±é“\n\n"
    "6. å±é™ºç‰©ï¼šæ¯’ç‰©ã€é•æ³•è–¬ç‰©ã€æ­¦å™¨ã€çˆ†ç™ºç‰©ã€çŠ¯ç½ªè¡Œç‚ºã«é–¢ã™ã‚‹æƒ…å ±\n"
    "   NGä¾‹: éŠƒå™¨ã®è²©å£²ã€å±é™ºãƒ‰ãƒ©ãƒƒã‚°ã®ç´¹ä»‹ã€ãƒãƒƒã‚­ãƒ³ã‚°æ–¹æ³•ã®å…·ä½“ä¾‹\n"
    "   OKä¾‹: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•“ç™ºã€åˆæ³•çš„ãªè­·èº«å…·ã®ç´¹ä»‹\n\n"
    "7. ã‚°ãƒ­ãƒ†ã‚¹ã‚¯ï¼šã‚·ãƒ§ãƒƒã‚­ãƒ³ã‚°ãªè¡¨ç¾ã€æ­»ä½“ã€è§£ä½“æ˜ åƒã€è‡ªæ®ºäº‹ä»¶ãªã©\n"
    "   NGä¾‹: è‡ªæ®ºç¾å ´ã®å†™çœŸã€æ˜†è™«é£Ÿæ˜ åƒã€æ’æ³„ç‰©ã€äº‹æ•…æ­»æ˜ åƒ\n"
    "   OKä¾‹: åŒ»å­¦çš„ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€èŠ¸è¡“çš„ãªå†™çœŸä½œå“\n\n"
    "8. ãƒã‚¬ãƒ†ã‚£ãƒ–ï¼šä¸»ã«ãƒ˜ã‚¤ãƒˆã«è©²å½“ã—ãªã„ãŒã€æ„šç—´ãƒ»æ‚²è¦³ãƒ»ä¸å®‰ã‚’ç…½ã‚‹å†…å®¹\n"
    "   NGä¾‹: æœ‰åäººä»¥å¤–ã¸ã®èª¹è¬—ã€å€‹äººã¸ã®ãƒã‚¬ãƒ†ã‚£ãƒ–æŠ•ç¨¿ã€æ—¥è¨˜å½¢å¼ã®æ„šç—´\n"
    "   OKä¾‹: ä¸€èˆ¬çš„ãªä¸æº€å…±æœ‰ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¸ã®ã‚³ãƒ¡ãƒ³ãƒˆ\n\n"
    "9. é–²è¦§ä¸å¯ï¼šã‚¢ã‚¯ã‚»ã‚¹ä¸å¯ã€403/404ã‚¨ãƒ©ãƒ¼ã€å†…å®¹ãŒç©ºã®å ´åˆ\n"
    "10. èªè¨¼ãŒå¿…è¦ï¼šãƒ­ã‚°ã‚¤ãƒ³ã—ãªã„ã¨é–²è¦§ã§ããªã„ã€ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰è¦æ±‚\n"
    "11. æµ·å¤–ã‚µã‚¤ãƒˆï¼šå¤–å›½èªã§æ›¸ã‹ã‚Œã¦ã„ã‚‹ã€å¤–å›½IPã‹ã‚‰é‹å–¶ã•ã‚Œã¦ã„ã‚‹ã‚µã‚¤ãƒˆ\n\n"
    f"{full_context}\n\n"
    f"[ç”»åƒã®èª¬æ˜ãƒ»OCRçµæœ]:\n{image_desc}\n\n"
    "ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘\n"
    "[ã‚¸ãƒ£ãƒ³ãƒ«]: â—‹â—‹ï¼ˆè¤‡æ•°ã‚ã‚Œã°ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰\n"
    "[ç†ç”±]: ã‚¸ãƒ£ãƒ³ãƒ«åˆ¤å®šã®æ ¹æ‹ ã‚’ç°¡æ½”ã«è¨˜è¼‰\n"
)
    try:
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": genre_prompt}],
            max_tokens=200,
            temperature=0.2,
        )
        content = response.choices[0].message.content.strip()
        genre_match = re.search(r"\[ã‚¸ãƒ£ãƒ³ãƒ«\]:\s*([^\n/]+)", content)
        reason_match = re.search(r"\[ç†ç”±\]:\s*(.+)", content)

        if genre_match:
            genre_text = genre_match.group(1).strip()
            if any(x in genre_text for x in ["è¦ç¢ºèª", "è©²å½“ãªã—", "ã‚«ãƒ†ã‚´ãƒªãƒ¼è©²å½“ãªã—"]):
                return "[ã‚¸ãƒ£ãƒ³ãƒ«]: ã‚«ãƒ†ã‚´ãƒªãƒ¼è©²å½“ãªã—"
            return f"[ã‚¸ãƒ£ãƒ³ãƒ«]: {genre_text} / [ç†ç”±]: {reason_match.group(1).strip() if reason_match else 'ãªã—'}"

        return "[ã‚¸ãƒ£ãƒ³ãƒ«]: ã‚«ãƒ†ã‚´ãƒªãƒ¼è©²å½“ãªã—"
    except Exception as e:
        return f"GPTã‚¨ãƒ©ãƒ¼: {str(e)}"

def gpt_judge_image(image_path, alt_ocr_text=""):
    if not GPT_API_KEY or not os.path.exists(image_path):
        return "ç”»åƒæœªè§£æ"
    try:
        with open(image_path, "rb") as f:
            img_bytes = f.read()
        with Image.open(image_path) as img:
            img = img.convert("RGB")
            img.thumbnail((1600, 1600))  # ãƒªã‚µã‚¤ã‚º
            buffered = io.BytesIO()
            img.save(buffered, format="PNG")
            img_bytes = buffered.getvalue()
        base64_image = base64.b64encode(img_bytes).decode("utf-8")

        # base64 å¤§ãã•ã®ã¯ã¿å‡ºã—é˜²æ­¢
        if len(base64_image) > 4_000_000:
            return f"ç”»åƒã‚µã‚¤ã‚ºéå¤§ï¼ˆ{len(base64_image)}ãƒã‚¤ãƒˆï¼‰ã§GPTä¸å¯"

        messages = [
            {"role": "user", "content":
            "ä»¥ä¸‹ã®ç”»åƒã¨ãã®OCR/ALTãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã€è©²å½“ã™ã‚‹ã‚¸ãƒ£ãƒ³ãƒ«ãŒã‚ã‚Œã°åˆ¤å®šã—ã¦ãã ã•ã„ã€‚\n"
            "æ˜ç¢ºã«è©²å½“ã—ãªã„å ´åˆã®ã¿ [è¦ç¢ºèª] ã¨ã—ã€å°‘ã—ã§ã‚‚è©²å½“ã®å¯èƒ½æ€§ãŒã‚ã‚Œã°è©²å½“ã‚¸ãƒ£ãƒ³ãƒ«ã‚’æŒ™ã’ã¦ãã ã•ã„ã€‚ã‚¸ãƒ£ãƒ³ãƒ«åã®å¾Œã‚ã«è£œè¶³ãŒã‚ã£ã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚ï¼ˆä¾‹: ã‚¢ãƒ€ãƒ«ãƒˆï¼ˆç–‘ã„ã‚ã‚Šï¼‰ï¼‰\n\n"
            "ã€ã‚¸ãƒ£ãƒ³ãƒ«å®šç¾©ã€‘\n"
            "- é–²è¦§ä¸å¯ï¼šé–²è¦§ãŒåˆ¶é™ã•ã‚Œã¦ã„ã‚‹è¡¨ç¤º\n"
            "- èªè¨¼ãŒå¿…è¦ï¼šãƒ­ã‚°ã‚¤ãƒ³ã‚’æ±‚ã‚ã‚‹ãƒšãƒ¼ã‚¸\n"
            "- ã‚¢ãƒ€ãƒ«ãƒˆï¼šR18ã€æ€§çš„ãªç”»åƒãƒ»è­¦å‘Šè¡¨ç¤º\n"
            "- æ‚ªè³ªCGMï¼šæ²ç¤ºæ¿ãƒ»ä¸é©åˆ‡ãªã‚³ãƒ¡ãƒ³ãƒˆå¤šæ•°\n"
            "- è‘—ä½œæ¨©ä¾µå®³ï¼šå‹•ç”»ãƒ»ç”»åƒãƒ»æ¼«ç”»ãªã©ã®ç„¡æ–­è»¢è¼‰\n"
            "- ãƒã‚¤ãƒ³ãƒˆï¼šå ±é…¬ä»˜ãåºƒå‘Šã€ãƒã‚¤ãƒ³ãƒˆäº¤æ›æ¡ˆå†…\n"
            "- ãƒ˜ã‚¤ãƒˆï¼šå·®åˆ¥è¡¨ç¾ã‚„ä¸­å‚·ãŒã‚ã‚‹\n"
            "- å±é™ºç‰©ï¼šè–¬ç‰©ãƒ»æ¯’ç‰©ãƒ»æš´åŠ›çš„ãªæå†™\n"
            "- ã‚°ãƒ­ãƒ†ã‚¹ã‚¯ï¼šæ­»ä½“ãƒ»äº‹æ•…ãƒ»è‡ªæ®ºãªã©\n"
            "- ãƒã‚¬ãƒ†ã‚£ãƒ–ï¼šæ‚²æƒ¨ãªå†…å®¹ã€çµ¶æœ›çš„è¡¨ç¾\n"
            "- æµ·å¤–ã‚µã‚¤ãƒˆï¼šè‹±èªãªã©å¤–å›½èªè¡¨ç¤ºã®ã‚‚ã®\n\n"
            f"[OCR/ALTãƒ†ã‚­ã‚¹ãƒˆã®ä¸€éƒ¨]: {alt_ocr_text[:200]}\n"
            "ã€å‡ºåŠ›å½¢å¼ã€‘\n"
            "[ã‚¸ãƒ£ãƒ³ãƒ«]: â—‹â—‹ / [ç†ç”±]: â—‹â—‹â—‹â—‹"
            },
            {"role": "user", "content": [
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}
            ]}
        ]
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=200,
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"ç”»åƒGPTã‚¨ãƒ©ãƒ¼: {str(e)}"

# ã‚µã‚¤ãƒˆåˆ¤æ–­
def extract_body_text(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    body = soup.body
    if not body:
        return ""
    for selector in ['header', 'footer', 'nav', '.footer', '.header', '.ads', '[id*="ad"]', '[class*="ad"]']:
        for tag in body.select(selector):
            tag.decompose()
    for tag in body(["script", "style"]):
        tag.decompose()
    return body.get_text(separator="\n", strip=True)

def is_japanese_site_by_html(html: str, threshold: float = 0.4) -> bool:
    text = extract_body_text(html)
    if not text:
        return False
    total_chars = len(text)
    jp_chars = len(re.findall(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯]', text))
    return (jp_chars / total_chars) >= threshold if total_chars > 0 else False


def is_japanese_site_by_html_or_ocr(url: str, html: str, ocr_text: str, threshold: float = 0.4) -> bool:
    domain = extract_domain(url)
    if any(domain.endswith(jd) for jd in JAPANESE_DOMAINS):
        return True

    # HTML BodyæŠ½å‡º
    text = extract_body_text(html)
    
    # HTML BodyãŒå°‘ãªã„å ´åˆ â†’ OCR ãƒ†ã‚­ã‚¹ãƒˆã§å¤‰ã‚ã£ã¦åˆ¤å®š
    if not text or len(text.strip()) < 30:
        text = ocr_text or ""
    
    # æ—¥æœ¬èªæ–‡å­—ã®å‰²åˆè¨ˆç®—ï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ï¼‰
    total_chars = len(text)
    jp_chars = len(re.findall(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯]', text))

    # è‹±èªã—ã‹ãªã‘ã‚Œã°æµ·å¤–ã¨ã¿ãªã™ (é€†è£œæ­£)
    en_chars = len(re.findall(r'[a-zA-Z]', text))
    if total_chars > 0 and jp_chars == 0 and en_chars > total_chars * 0.2:
        return False

    return (jp_chars / total_chars) >= threshold if total_chars > 0 else False


# ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
def compute_integrated_score(ocr_match_count, gpt_result_text, gpt_image_text):
    score = 0
    if ocr_match_count >= 4:
        score += 5
    elif ocr_match_count >= 2:
        score += 2.5
    elif ocr_match_count >= 1:
        score += 1.5
    if any(g in gpt_result_text for g in ['ã‚¢ãƒ€ãƒ«ãƒˆ', 'æ‚ªè³ªCGM', 'è‘—ä½œæ¨©ä¾µå®³', 'ãƒ˜ã‚¤ãƒˆ', 'å±é™ºç‰©', 'ã‚°ãƒ­ãƒ†ã‚¹ã‚¯']):
        score += 5
    gpt_image_genre_match = re.search(r"\[ã‚¸ãƒ£ãƒ³ãƒ«\]:\\s*(.+?)\\s*(/|\\n|$)", gpt_image_text)
    if gpt_image_genre_match:
        gpt_image_genre = gpt_image_genre_match.group(1).strip()
        if gpt_image_genre in ['é–²è¦§ä¸å¯', 'èªè¨¼ãŒå¿…è¦', 'ã‚¢ãƒ€ãƒ«ãƒˆ', 'æ‚ªè³ªCGM', 'è‘—ä½œæ¨©ä¾µå®³', 'ãƒ˜ã‚¤ãƒˆ', 'å±é™ºç‰©', 'ã‚°ãƒ­ãƒ†ã‚¹ã‚¯']:
            score += 5
    return round(min(score, 15), 1)

# Googleãƒ‰ãƒ©ã‚¤ãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
def upload_to_drive(file_path: str, file_name: str) -> str:
    try:
        # âœ… ì—…ë¡œë“œí•  íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„° ì •ì˜ (í´ë” ì§€ì •)
        file_metadata = {
            'name': file_name,
            'parents': [DRIVE_FOLDER_ID]  # ê³µìœ ë“œë¼ì´ë¸Œ ë‚´ í´ë” ID
        }

        # âœ… ì‹¤ì œ íŒŒì¼ ì¤€ë¹„
        media = MediaFileUpload(file_path, resumable=True)

        # âœ… Google Driveì— íŒŒì¼ ì—…ë¡œë“œ ì‹¤í–‰
        file = drive_service.files().create(
            body=file_metadata,
            media_body=media,
            fields='id',
            supportsAllDrives=True
        ).execute()

        file_id = file.get('id')

        # âœ… ê³µê°œ ê¶Œí•œ ë¶€ì—¬ (ê³µìœ ë“œë¼ì´ë¸Œë©´ ì‹¤íŒ¨í•  ìˆ˜ë„ ìˆìŒ â†’ ë¬´ì‹œ)
        try:
            drive_service.permissions().create(
                fileId=file_id,
                body={"role": "reader", "type": "anyone"},
                supportsAllDrives=True
            ).execute()
        except Exception as e:
            st.warning(f"âš ï¸ å…¬é–‹æ¨©é™ã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸãŒã€è¦ªãƒ•ã‚©ãƒ«ãƒ€ã®è¨­å®šã«ã‚ˆã‚Šé–²è¦§å¯èƒ½ãªå ´åˆã¯å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚\n{e}")

        # âœ… Google Sheetsì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê³µê°œ ì´ë¯¸ì§€ ë§í¬ ë°˜í™˜
        return f"https://drive.google.com/uc?id={file_id}"

    except Exception as e:
        st.error(f"âŒ Drive ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
        return ""


# Streamlit UI
st.title("Web Unsafe åŠå®š")
urls_input = st.text_area("å¯¾è±¡URLä¸€è¦§", height=200)

if st.button("åˆ¤å®šå®Ÿè¡Œ"):
    urls = [u.strip() for u in urls_input.strip().split('\n') if u.strip()]
    progress = st.progress(0)
    status_text = st.empty()

    start_time = time.time()

    for idx, url in enumerate(urls, 1):
        try:
            maintext, image_desc, shot_path, html, ocr_text = crawl_with_ocr(url, idx)

            gpt_image_opinion = gpt_judge_image(shot_path, image_desc) if shot_path and os.path.isfile(shot_path) else "ç”»åƒæœªè§£æ"
            gpt_opinion = gpt_judge_genre(maintext, image_desc, GENRE_KEYWORDS) if GPT_API_KEY else "GPTæœªå®Ÿè¡Œ"

            if "è¦ç¢ºèª" in gpt_image_opinion or "åˆ¤å®šã§ãã¾ã›ã‚“" in gpt_image_opinion or "æœªè§£æ" in gpt_image_opinion:
                preferred_opinion = gpt_opinion
            else:
                preferred_opinion = gpt_image_opinion

            drive_url = upload_to_drive(shot_path, os.path.basename(shot_path)) if shot_path and os.path.isfile(shot_path) else "çµæœãªã—"
            keyword_result = judge_keywords_by_count(maintext + " " + image_desc, GENRE_KEYWORDS)
            keyword_summary = "\n".join(f"{genre}ï¼ˆ{count}å€‹ï¼‰: {', '.join(matched)}" for genre, count, matched in keyword_result)

            ocr_kw_count = sum(len(matched) for _, _, matched in keyword_result)
            ocr_point = 5 if ocr_kw_count >= 5 else 4 if ocr_kw_count >= 4 else 3 if ocr_kw_count >= 3 else 2 if ocr_kw_count >= 1 else 0
            gpt_point = 5 if any(g in gpt_opinion for g in ['ã‚¢ãƒ€ãƒ«ãƒˆ', 'æ‚ªè³ªCGM', 'è‘—ä½œæ¨©ä¾µå®³', 'ãƒ˜ã‚¤ãƒˆ', 'å±é™ºç‰©', 'ã‚°ãƒ­ãƒ†ã‚¹ã‚¯']) else 0
            gpt_image_point = 0
            genre_match = re.search(r"\[ã‚¸ãƒ£ãƒ³ãƒ«\]:\s*(.+?)\s*(/|\n|$)", gpt_image_opinion)
            if genre_match and genre_match.group(1).strip() in ['é–²è¦§ä¸å¯', 'èªè¨¼ãŒå¿…è¦', 'ã‚¢ãƒ€ãƒ«ãƒˆ', 'æ‚ªè³ªCGM', 'è‘—ä½œæ¨©ä¾µå®³', 'ãƒ˜ã‚¤ãƒˆ', 'å±é™ºç‰©', 'ã‚°ãƒ­ãƒ†ã‚¹ã‚¯']:
                gpt_image_point = 5

            final_score = min(ocr_point + gpt_point + gpt_image_point, 15)

            cell = worksheet.find(url)
            row = cell.row

            site_origin = "æ—¥æœ¬ã‚µã‚¤ãƒˆ" if is_japanese_site_by_html_or_ocr(url, html, ocr_text) else "æµ·å¤–ã‚µã‚¤ãƒˆ"

            score_explanation = (
                f"{site_origin}\n"
                f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢: {ocr_point}ç‚¹\n"
                f"GPTæœ¬æ–‡ã‚¹ã‚³ã‚¢: {gpt_point}ç‚¹\n"
                f"GPTç”»åƒã‚¹ã‚³ã‚¢: {gpt_image_point}ç‚¹\n"
                f"[æœ€çµ‚ã‚¹ã‚³ã‚¢]: {final_score}/15"
            )

            risk_level = "Unsafe" if final_score >= 11 else "NotSafe" if final_score >= 5 else "Safe"

            worksheet.update_cell(row, 2, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))  # B
            worksheet.update_cell(row, 18, maintext[:1200])   # R
            worksheet.update_cell(row, 19, f'=IMAGE("{drive_url}", 1)')  # S
            worksheet.update_cell(row, 20, gpt_opinion)        # T
            worksheet.update_cell(row, 21, gpt_image_opinion)  # U
            worksheet.update_cell(row, 22, keyword_summary)    # V
            worksheet.update_cell(row, 23, score_explanation)  # W
            worksheet.update_cell(row, 24, risk_level)         # X
            # --- [Yåˆ—: ã‚¸ãƒ£ãƒ³ãƒ«åˆ†é¡] åæ˜ å‡¦ç† --- #
            genre_final_match = re.search(r"\[ã‚¸ãƒ£ãƒ³ãƒ«\]:\s*([^\n]+)", preferred_opinion)
            if genre_final_match:
                genre_final = genre_final_match.group(1).strip()
                if any(x in genre_final for x in ["è¦ç¢ºèª", "è©²å½“ãªã—", "ã‚«ãƒ†ã‚´ãƒªãƒ¼è©²å½“ãªã—"]):
                    genre_final = "ã‚«ãƒ†ã‚´ãƒªãƒ¼è©²å½“ãªã—"
            else:
                genre_final = "ã‚«ãƒ†ã‚´ãƒªãƒ¼è©²å½“ãªã—"

            worksheet.update_cell(row, 25, genre_final)  # âœ… Yåˆ—

        except Exception as e:
            st.error(f"[{idx}] {url} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
        finally:
            if shot_path and os.path.isfile(shot_path):
                try:
                    os.remove(shot_path)
                except:
                    pass

            elapsed = time.time() - start_time
            avg_time = elapsed / idx
            remaining = avg_time * (len(urls) - idx)
            rem_min, rem_sec = divmod(int(remaining), 60)
            status_text.text(f"é€²è¡Œä¸­: {idx}/{len(urls)}ä»¶ â³ æ®‹ã‚Šäºˆæƒ³: {rem_min}åˆ†{rem_sec}ç§’")
            progress.progress(idx / len(urls))

    total_time = time.time() - start_time
    total_min, total_sec = divmod(int(total_time), 60)
    st.success(f"åˆ¤å®šå®Œäº† âœ… æ‰€è¦æ™‚é–“: {total_min}åˆ†{total_sec}ç§’")